---
title: "Phase diagram and eigenvalue dynamics of stochastic gradient descent in multilayer neural networks"
authors:
  - "Chanju Park"
  - "Biagio Lucini"
  - "Gert Aarts"
year: 2025
journal: "Machine Learning: Science and Technology"
doi: "10.1088/2632-2153/ae1acc"
url: "https://doi.org/10.1088/2632-2153/ae1acc"
lab: "radboud-university"
faculty:
  - "Esther Aarts"
tags:
  - "publication"
  - "radboud-university"
abstract: |
  <jats:title>Abstract</jats:title>
                    <jats:p>Hyperparameter tuning is one of the essential steps to guarantee the convergence of machine learning models. We argue that intuition about the optimal choice of hyperparameters for stochastic gradient descent can be obtained by studying a neural network’s phase diagram, in which each phase is characterised by distinctive dynamics of the singular values of weight matrices. Taking inspiration from disordered systems, we start from the observation that the loss landscape of a multilayer neural network with mean squared error can be interpreted as a disordered system in feature space, where the learnt features are mapped to soft spin degrees of freedom, the initial variance of the weight matrices is interpreted as the strength of the disorder, and temperature is given by the ratio of the learning rate and the batch size. As the model is trained, three phases can be identified, in which the dynamics of weight matrices is qualitatively different. Employing a Langevin equation for stochastic gradient descent, previously derived using Dyson Brownian motion, we demonstrate that the three dynamical regimes can be classified effectively, providing practical guidance for the choice of hyperparameters of the optimiser.</jats:p>
fulltext_available: false
fulltext_source: "none"
created: "2025-11-24T09:31:00.273321"
---

# Phase diagram and eigenvalue dynamics of stochastic gradient descent in multilayer neural networks

## Abstract

<jats:title>Abstract</jats:title>
                  <jats:p>Hyperparameter tuning is one of the essential steps to guarantee the convergence of machine learning models. We argue that intuition about the optimal choice of hyperparameters for stochastic gradient descent can be obtained by studying a neural network’s phase diagram, in which each phase is characterised by distinctive dynamics of the singular values of weight matrices. Taking inspiration from disordered systems, we start from the observation that the loss landscape of a multilayer neural network with mean squared error can be interpreted as a disordered system in feature space, where the learnt features are mapped to soft spin degrees of freedom, the initial variance of the weight matrices is interpreted as the strength of the disorder, and temperature is given by the ratio of the learning rate and the batch size. As the model is trained, three phases can be identified, in which the dynamics of weight matrices is qualitatively different. Employing a Langevin equation for stochastic gradient descent, previously derived using Dyson Brownian motion, we demonstrate that the three dynamical regimes can be classified effectively, providing practical guidance for the choice of hyperparameters of the optimiser.</jats:p>

## Links

- DOI: [10.1088/2632-2153/ae1acc](https://doi.org/10.1088/2632-2153/ae1acc)
- URL: [Link](https://doi.org/10.1088/2632-2153/ae1acc)

## Faculty

- [[radboud-university/faculty#esther-aarts|Esther Aarts]]

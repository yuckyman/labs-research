---
title: "Combining complex Langevin dynamics with score-based and energy-based diffusion models"
authors:
  - "Gert Aarts"
  - "Diaa E. Habibi"
  - "Lingxiao Wang"
  - "Kai Zhou"
year: 2025
journal: "Journal of High Energy Physics"
doi: "10.1007/jhep12(2025)160"
url: "https://doi.org/10.1007/jhep12(2025)160"
lab: "radboud-university"
faculty:
  - "Esther Aarts"
tags:
  - "publication"
  - "radboud-university"
abstract: |
  <jats:title>
                      A
                      <jats:sc>bstract</jats:sc>
                    </jats:title>
                    <jats:p>
                      Theories with a sign problem due to a complex action or Boltzmann weight can sometimes be numerically solved using a stochastic process in the complexified configuration space. However, the probability distribution effectively sampled by this complex Langevin process is not known
                      <jats:italic>a priori</jats:italic>
                      and notoriously hard to understand. In generative AI, diffusion models can learn distributions, or their log derivatives, from data. We explore the ability of diffusion models to learn the distributions sampled by a complex Langevin process, comparing score-based and energy-based diffusion models, and speculate about possible applications.
                    </jats:p>
fulltext_available: false
fulltext_source: "none"
created: "2025-12-29T09:34:45.264157"
---

# Combining complex Langevin dynamics with score-based and energy-based diffusion models

## Abstract

<jats:title>
                    A
                    <jats:sc>bstract</jats:sc>
                  </jats:title>
                  <jats:p>
                    Theories with a sign problem due to a complex action or Boltzmann weight can sometimes be numerically solved using a stochastic process in the complexified configuration space. However, the probability distribution effectively sampled by this complex Langevin process is not known
                    <jats:italic>a priori</jats:italic>
                    and notoriously hard to understand. In generative AI, diffusion models can learn distributions, or their log derivatives, from data. We explore the ability of diffusion models to learn the distributions sampled by a complex Langevin process, comparing score-based and energy-based diffusion models, and speculate about possible applications.
                  </jats:p>

## Links

- DOI: [10.1007/jhep12(2025)160](https://doi.org/10.1007/jhep12(2025)160)
- URL: [Link](https://doi.org/10.1007/jhep12(2025)160)

## Faculty

- [[radboud-university/faculty#esther-aarts|Esther Aarts]]
